{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types \n",
    "from pyspark.sql.types import DateType, StringType\n",
    "from pyspark.sql.functions import regexp_extract, udf\n",
    "from pyspark.sql.functions import year, month, col, sum, udf, substring, split, regexp_replace, when, lower, upper, countDistinct\n",
    "from pyspark.sql.functions import isnan, count, lit\n",
    "import glob as gb\n",
    "import glob\n",
    "#from tqdm import tqdm\n",
    "import re \n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://julianes-mbp.station:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SRAG_cases</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd8e5f0fc10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "sc.stop()\n",
    "\n",
    "conf = SparkConf().setAppName(\"SRAG_cases\")\n",
    "\n",
    "conf = (conf.setMaster(\"local[*]\")\n",
    "       .set(\"spark.executor.memory\", \"3GB\")\n",
    "       .set(\"spark.driver.memory\", \"20GB\"))\n",
    "\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define paths to files\n",
    "spark = SparkSession.builder.appName(\"SRAG\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get path names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD11.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD10.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD12.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD13.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD17.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD16.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD14.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD15.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD18.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD19.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD22-12-12-2022.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD09.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD20-12-12-2022.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD21-12-12-2022.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and get dataframe columns for variables' harmonization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD11.csv\n",
      ",\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD10.csv\n",
      ",\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD12.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD13.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD17.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD16.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD14.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD15.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD18.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD19.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD22-12-12-2022.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD09.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD20-12-12-2022.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD21-12-12-2022.csv\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "dataframes = []\n",
    "lst_df_columns = []\n",
    "for f in filenames:\n",
    "    \n",
    "    print(f)\n",
    "    \n",
    "    #Get dataframe delimiter\n",
    "    delimiter=spark.createDataFrame(sc.textFile(f).take(1),StringType()).\\\n",
    "    withColumn(\"chars\",regexp_extract(col(\"value\"),\"(,|;|\\\\|)\",1)).\\\n",
    "    select(\"chars\").\\\n",
    "    collect()[0][0]\n",
    "    \n",
    "    print(delimiter)\n",
    "    \n",
    "    df = spark.read.\\\n",
    "    option(\"delimiter\",delimiter).\\\n",
    "    option(\"header\",True).\\\n",
    "    csv(f)\n",
    "    \n",
    "    dataframes.append(df)\n",
    "    lst_df_columns.append([f[-17:]] + df.columns)\n",
    "    \n",
    "# Old method\n",
    "#lst_df_columns = []\n",
    "#for f in filenames:\n",
    "#    print(f)\n",
    "#    df = pd.read_csv(f, low_memory = False, encoding=\"iso-8859-1\")  \n",
    "#    df = spark.read.format(\"csv\").option(\"header\",True).load(f)\n",
    "#    lst_df_columns.append([f[-17:]] + df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.parallelize([lst_df_columns]).saveAsTextFile(\"/Users/julianeoliveira/Documents/Projects/AESOP/Documentation - Data on Respiratory diseases/dic_srag.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search variable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_variables(variable): \n",
    "\n",
    "    for value in lst_df_columns:\n",
    "        print(value[0], variable in value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRAG/INFLUD11.csv True\n",
      "SRAG/INFLUD10.csv False\n",
      "SRAG/INFLUD12.csv True\n",
      "SRAG/INFLUD13.csv True\n",
      "SRAG/INFLUD17.csv True\n",
      "SRAG/INFLUD16.csv True\n",
      "SRAG/INFLUD14.csv True\n",
      "SRAG/INFLUD15.csv True\n",
      "SRAG/INFLUD18.csv True\n",
      "SRAG/INFLUD19.csv False\n",
      "22-12-12-2022.csv False\n",
      "SRAG/INFLUD09.csv True\n",
      "20-12-12-2022.csv False\n",
      "21-12-12-2022.csv False\n"
     ]
    }
   ],
   "source": [
    "check_variables('NU_ANO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT NU_ANO)|\n",
      "+----------------------+\n",
      "|                     2|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframes[0].select(countDistinct('NU_ANO')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|NU_ANO|count|\n",
      "+------+-----+\n",
      "|  2012|   82|\n",
      "|  2011| 4333|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframes[0].groupBy('NU_ANO').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[1] = dataframes[1].withColumn(\"ano\", lit(2010))\n",
    "\n",
    "dataframes[9] = dataframes[9].withColumn(\"ano\", lit(2019))\n",
    "\n",
    "dataframes[10] = dataframes[10].withColumn(\"ano\", lit(2022))\n",
    "\n",
    "dataframes[12] = dataframes[12].withColumn(\"ano\", lit(2020))\n",
    "\n",
    "dataframes[13] = dataframes[13].withColumn(\"ano\", lit(2021))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[1].select('ano').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRAG/INFLUD11.csv 4415\n",
      "SRAG/INFLUD10.csv 11318\n",
      "SRAG/INFLUD12.csv 21163\n",
      "SRAG/INFLUD13.csv 36563\n",
      "SRAG/INFLUD17.csv 29551\n",
      "SRAG/INFLUD16.csv 54380\n",
      "SRAG/INFLUD14.csv 18996\n",
      "SRAG/INFLUD15.csv 14553\n",
      "SRAG/INFLUD18.csv 47756\n",
      "SRAG/INFLUD19.csv 48528\n",
      "22-12-12-2022.csv 516626\n",
      "SRAG/INFLUD09.csv 88354\n",
      "20-12-12-2022.csv 1200995\n",
      "21-12-12-2022.csv 1733910\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(dataframes)):\n",
    "    print(lst_df_columns[i][0], dataframes[i].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distinct_variable(lst_dfs,var):\n",
    "    for i in range(0,len(lst_dfs)):\n",
    "        print(lst_df_columns[i][0], lst_dfs[i].select(countDistinct(var)).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_group_variable(lst_dfs,var):\n",
    "    for i in range(0,len(lst_dfs)):\n",
    "        print(lst_df_columns[i][0], lst_dfs[i].groupBy(var).count().show(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_null(df):\n",
    "    Dict_Null = {col:df.filter(df[col].isNull()).count() for col in df.columns}\n",
    "    print(Dict_Null)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and extract variables from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_select = ['sg_uf_not', 'id_municip','sg_uf', 'id_mn_resi',\n",
    "                 'dt_notific','sem_not', 'nu_ano','dt_sin_pri','sem_pri',\n",
    "                  'classi_fin','criterio', 'co_mun_not']\n",
    "\n",
    "#id_municip - Município onde está localizada a Unidade Sentinela que realizou a notificação. \n",
    "# This variable is substituted by 'co_mun_not' for the years of 2019 and so\n",
    "\n",
    "# Number of states that reported a case\n",
    "#count_distinct_variable(dataframes,\"sg_uf_not\")\n",
    "\n",
    "# Number of cases by UF of notification\n",
    "#count_group_variable(dataframes,\"sg_uf_not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_variables('CO_MUN_NOT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert columns name to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0, len(dataframes)):\n",
    "    df = dataframes[j]\n",
    "    for col in df.columns:\n",
    "        df = df.withColumnRenamed(col, col.lower())\n",
    "        dataframes[j] = df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Municipality of residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,9):\n",
    "    if \"id_mn_resi\" in dataframes[i].columns:\n",
    "        dataframes[i] = dataframes[i].withColumnRenamed(\"id_mn_resi\", \"codmunres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[11] = dataframes[11].withColumnRenamed(\"id_mn_resi\", \"codmunres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    if \"co_mun_res\" in dataframes[i].columns:\n",
    "        dataframes[i] = dataframes[i].withColumnRenamed(\"co_mun_res\", \"codmunres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Municipality of notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,9):\n",
    "    if \"id_municip\" in dataframes[i].columns:\n",
    "        dataframes[i] = dataframes[i].withColumnRenamed(\"id_municip\", \"codmunnot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[11] = dataframes[11].withColumnRenamed(\"id_municip\", \"codmunnot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    if \"co_mun_not\" in dataframes[i].columns:\n",
    "        dataframes[i] = dataframes[i].withColumnRenamed(\"co_mun_not\", \"codmunnot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    if \"nu_ano\" in dataframes[i].columns:\n",
    "        dataframes[i] = dataframes[i].withColumnRenamed(\"nu_ano\", \"ano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'codmunnot' in dataframes[0].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select variables from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    \n",
    "    cols = ['ano','sg_uf','sg_uf_not','codmunres','codmunnot','dt_sin_pri','dt_notific','sem_not','classi_fin','criterio']\n",
    "    \n",
    "    df_new = dataframes[i].select(*cols)\n",
    "    \n",
    "    dfs.append(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+---------+---------+----------+----------+-------+----------+--------+\n",
      "| ano|sg_uf|sg_uf_not|codmunres|codmunnot|dt_sin_pri|dt_notific|sem_not|classi_fin|criterio|\n",
      "+----+-----+---------+---------+---------+----------+----------+-------+----------+--------+\n",
      "|2010|   11|       11|   110002|   110020|15/04/2010|21/04/2010| 201016|       3.0|    null|\n",
      "|2010|   11|       11|   110002|   110020|30/04/2010|04/05/2010| 201018|       1.0|     1.0|\n",
      "|2010|   11|       11|   110002|   110020|03/05/2010|11/05/2010| 201019|       3.0|     1.0|\n",
      "|2010|   11|       11|   110004|   110020|08/03/2010|22/03/2010| 201012|       1.0|     2.0|\n",
      "|2010|   11|       11|   110010|   110020|28/01/2010|02/02/2010| 201005|       3.0|     1.0|\n",
      "|2010|   11|       11|   110010|   110010|14/03/2010|17/03/2010| 201011|       3.0|     2.0|\n",
      "|2010|   11|       11|   110010|   110010|03/03/2010|07/03/2010| 201010|       3.0|    null|\n",
      "|2010|   11|       11|   110010|   110010|18/04/2010|22/04/2010| 201016|       3.0|     2.0|\n",
      "|2010|   11|       11|   110010|   110010|10/04/2010|14/04/2010| 201015|       3.0|     2.0|\n",
      "|2010|   11|       11|   110010|   110010|08/04/2010|13/04/2010| 201015|       3.0|     2.0|\n",
      "|2010|   11|       11|   110010|   110010|10/04/2010|14/04/2010| 201015|       3.0|    null|\n",
      "|2010|   11|       11|   110010|   110010|14/04/2010|14/04/2010| 201015|       3.0|    null|\n",
      "|2010|   11|       11|   110010|   110010|16/04/2010|16/04/2010| 201015|       3.0|    null|\n",
      "|2010|   11|       11|   110010|   110010|08/04/2010|12/04/2010| 201015|       3.0|     2.0|\n",
      "|2010|   11|       11|   110010|   110010|14/04/2010|18/04/2010| 201016|       3.0|    null|\n",
      "|2010|   11|       11|   110010|   110010|09/04/2010|13/04/2010| 201015|       3.0|    null|\n",
      "|2010|   11|       11|   110010|   110010|09/04/2010|13/04/2010| 201015|       3.0|     2.0|\n",
      "|2010|   11|       11|   110010|   110010|06/04/2010|10/04/2010| 201014|       3.0|     2.0|\n",
      "|2010|   11|       11|   110010|   110010|08/04/2010|12/04/2010| 201015|       3.0|     2.0|\n",
      "|2010|   11|       11|   110010|   110010|09/04/2010|13/04/2010| 201015|       3.0|     2.0|\n",
      "+----+-----+---------+---------+---------+----------+----------+-------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to combine all dfs\n",
    "def unionAll(dfs):\n",
    "    return reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = unionAll(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+---------+---------+----------+----------+-------+----------+--------+\n",
      "| ano|sg_uf|sg_uf_not|codmunres|codmunnot|dt_sin_pri|dt_notific|sem_not|classi_fin|criterio|\n",
      "+----+-----+---------+---------+---------+----------+----------+-------+----------+--------+\n",
      "|2011|   11|       11|   110020|   110020|15/02/2011|21/02/2011| 201108|       3.0|     1.0|\n",
      "|2011|   11|       53|   110020|   530010|29/03/2011|07/04/2011| 201114|       3.0|     1.0|\n",
      "|2011|   11|       11|   110020|   110020|28/04/2011|04/05/2011| 201118|       3.0|     1.0|\n",
      "|2011|   11|       11|   110020|   110020|23/08/2011|27/08/2011| 201134|       1.0|     1.0|\n",
      "|2011|   11|       11|   110012|   110020|18/08/2011|31/08/2011| 201135|       3.0|     1.0|\n",
      "|2011|   11|       11|   110020|   110020|01/08/2011|05/08/2011| 201131|       3.0|     1.0|\n",
      "|2011|   11|       11|   110011|   110020|19/08/2011|19/09/2011| 201138|       3.0|    null|\n",
      "|2011|   11|       11|   110011|   110020|06/10/2011|11/10/2011| 201141|       3.0|     1.0|\n",
      "|2011|   11|       11|   110009|   110004|05/09/2011|12/09/2011| 201137|       3.0|    null|\n",
      "|2011|   11|       11|   110020|   110020|25/11/2011|30/11/2011| 201148|       3.0|     1.0|\n",
      "|2011|   11|       11|   110020|   110020|09/11/2011|11/11/2011| 201145|       3.0|    null|\n",
      "|2011|   12|       12|   120010|   120040|16/06/2011|18/06/2011| 201124|       2.0|     1.0|\n",
      "|2011|   12|       12|   120040|   120040|12/06/2011|27/06/2011| 201126|       3.0|    null|\n",
      "|2011|   12|       12|   120025|   120010|06/06/2011|13/06/2011| 201124|       3.0|     1.0|\n",
      "|2011|   12|       12|   120025|   120010|08/06/2011|09/06/2011| 201123|       1.0|     1.0|\n",
      "|2011|   12|       12|   120010|   120010|11/06/2011|13/06/2011| 201124|       3.0|     1.0|\n",
      "|2011|   12|       12|   120040|   120040|22/08/2011|26/08/2011| 201134|       3.0|    null|\n",
      "|2011|   12|       12|   120040|   120040|18/08/2011|24/08/2011| 201134|       3.0|     1.0|\n",
      "|2011|   12|       12|   120040|   120040|18/08/2011|23/08/2011| 201134|       3.0|    null|\n",
      "|2011|   12|       12|   120040|   120040|28/08/2011|31/08/2011| 201135|       3.0|     1.0|\n",
      "+----+-----+---------+---------+---------+----------+----------+-------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3827108"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.coalesce(1).write.format(\"csv\").save(\"/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/Filtered_raw/srag_2009_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
