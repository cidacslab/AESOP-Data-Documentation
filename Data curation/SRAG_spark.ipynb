{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types \n",
    "from pyspark.sql.types import DateType, StringType\n",
    "from pyspark.sql.functions import regexp_extract, udf\n",
    "from pyspark.sql.functions import year, month, col, sum, udf, substring, split, regexp_replace, when, lower, upper, countDistinct\n",
    "import glob as gb\n",
    "import glob\n",
    "#from tqdm import tqdm\n",
    "import re \n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://julianes-mbp.station:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SRAG_cases</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd7ec60fc10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "sc.stop()\n",
    "\n",
    "conf = SparkConf().setAppName(\"SRAG_cases\")\n",
    "\n",
    "conf = (conf.setMaster(\"local[*]\")\n",
    "       .set(\"spark.executor.memory\", \"3GB\")\n",
    "       .set(\"spark.driver.memory\", \"20GB\"))\n",
    "\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define paths to files\n",
    "spark = SparkSession.builder.appName(\"SRAG\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get path names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD11.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD10.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD12.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD13.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD17.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD16.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD14.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD15.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD18.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD19.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD22-12-12-2022.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD09.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD20-12-12-2022.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD21-12-12-2022.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and get dataframe columns for variables' harmonization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD11.csv\n",
      ",\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD10.csv\n",
      ",\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD12.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD13.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD17.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD16.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD14.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD15.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD18.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD19.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD22-12-12-2022.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD09.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD20-12-12-2022.csv\n",
      ";\n",
      "/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD21-12-12-2022.csv\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "dataframes = []\n",
    "lst_df_columns = []\n",
    "for f in filenames:\n",
    "    \n",
    "    print(f)\n",
    "    \n",
    "    #Get dataframe delimiter\n",
    "    delimiter=spark.createDataFrame(sc.textFile(f).take(1),StringType()).\\\n",
    "    withColumn(\"chars\",regexp_extract(col(\"value\"),\"(,|;|\\\\|)\",1)).\\\n",
    "    select(\"chars\").\\\n",
    "    collect()[0][0]\n",
    "    \n",
    "    print(delimiter)\n",
    "    \n",
    "    df = spark.read.\\\n",
    "    option(\"delimiter\",delimiter).\\\n",
    "    option(\"header\",True).\\\n",
    "    csv(f)\n",
    "    \n",
    "    dataframes.append(df)\n",
    "    lst_df_columns.append([f[-17:]] + df.columns)\n",
    "    \n",
    "# Old method\n",
    "#lst_df_columns = []\n",
    "#for f in filenames:\n",
    "#    print(f)\n",
    "#    df = pd.read_csv(f, low_memory = False, encoding=\"iso-8859-1\")  \n",
    "#    df = spark.read.format(\"csv\").option(\"header\",True).load(f)\n",
    "#    lst_df_columns.append([f[-17:]] + df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.parallelize([lst_df_columns]).saveAsTextFile(\"/Users/julianeoliveira/Documents/Projects/AESOP/Documentation - Data on Respiratory diseases/dic_srag.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search variable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_variables(variable): \n",
    "\n",
    "    for value in lst_df_columns:\n",
    "        print(value[0], variable in value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRAG/INFLUD11.csv False\n",
      "SRAG/INFLUD10.csv False\n",
      "SRAG/INFLUD12.csv False\n",
      "SRAG/INFLUD13.csv False\n",
      "SRAG/INFLUD17.csv False\n",
      "SRAG/INFLUD16.csv False\n",
      "SRAG/INFLUD14.csv False\n",
      "SRAG/INFLUD15.csv False\n",
      "SRAG/INFLUD18.csv False\n",
      "SRAG/INFLUD19.csv True\n",
      "22-12-12-2022.csv True\n",
      "SRAG/INFLUD09.csv False\n",
      "20-12-12-2022.csv True\n",
      "21-12-12-2022.csv True\n"
     ]
    }
   ],
   "source": [
    "check_variables('CO_MUN_NOT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRAG/INFLUD11.csv 4415\n",
      "SRAG/INFLUD10.csv 11318\n",
      "SRAG/INFLUD12.csv 21163\n",
      "SRAG/INFLUD13.csv 36563\n",
      "SRAG/INFLUD17.csv 29551\n",
      "SRAG/INFLUD16.csv 54380\n",
      "SRAG/INFLUD14.csv 18996\n",
      "SRAG/INFLUD15.csv 14553\n",
      "SRAG/INFLUD18.csv 47756\n",
      "SRAG/INFLUD19.csv 48528\n",
      "22-12-12-2022.csv 516626\n",
      "SRAG/INFLUD09.csv 88354\n",
      "20-12-12-2022.csv 1200995\n",
      "21-12-12-2022.csv 1733910\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(dataframes)):\n",
    "    print(lst_df_columns[i][0], dataframes[i].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distinct_variable(lst_dfs,var):\n",
    "    for i in range(0,len(lst_dfs)):\n",
    "        print(lst_df_columns[i][0], lst_dfs[i].select(countDistinct(var)).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_group_variable(lst_dfs,var):\n",
    "    for i in range(0,len(lst_dfs)):\n",
    "        print(lst_df_columns[i][0], lst_dfs[i].groupBy(var).count().show(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and extract variables from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_select = ['sg_uf_not', 'id_municip','seg_uf', 'id_mn_resi',\n",
    "                 'dt_notific','sem_not', 'nu_ano','dt_sin_pri','sem_pri',\n",
    "                  'classi_fin','criterio', 'co_mun_not']\n",
    "\n",
    "#id_municip - Município onde está localizada a Unidade Sentinela que realizou a notificação. \n",
    "# This variable is substituted by 'co_mun_not' for the years of 2019 and so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert columns name to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0, len(dataframes)):\n",
    "    df = dataframes[j]\n",
    "    for col in df.columns:\n",
    "        df = df.withColumnRenamed(col, col.lower())\n",
    "        dataframes[j] = df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change variable names\n",
    "for df in dataframes:\n",
    "    if \"id_mn_resi\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"id_mn_resi\", \"codmunres\")\n",
    "        \n",
    "    if \"co_mun_res\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"co_mun_res\", \"codmunres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of states that reported a case\n",
    "#count_distinct_variable(dataframes,\"sg_uf_not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of cases by UF of notification\n",
    "#count_group_variable(dataframes,\"sg_uf_not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for df in dataframes:\n",
    "    \n",
    "    cols = ['sg_uf_not','id_municip']\n",
    "    \n",
    "    df_new = df.select(*cols)\n",
    "    \n",
    "    dfs.append(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'co_mun_not' and  'id_municip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD11.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD10.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD12.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD13.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD17.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD16.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD14.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD15.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD18.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD19.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD22-12-12-2022.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD09.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD20-12-12-2022.csv',\n",
       " '/Users/julianeoliveira/Documents/Projects/AESOP/AESOP datalake/SRAG/INFLUD21-12-12-2022.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|co_mun_not|\n",
      "+----------+\n",
      "|    310620|\n",
      "|    355030|\n",
      "|    261160|\n",
      "|    350950|\n",
      "|    260410|\n",
      "|    261160|\n",
      "|    500270|\n",
      "|    261160|\n",
      "|    261160|\n",
      "|    261160|\n",
      "|    352590|\n",
      "|    261160|\n",
      "|    330455|\n",
      "|    261160|\n",
      "|    261160|\n",
      "|    330455|\n",
      "|    261160|\n",
      "|    261160|\n",
      "|    261160|\n",
      "|    354980|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframes[9].select('co_mun_not').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         id_municip|\n",
      "+-------------------+\n",
      "|             MACEIO|\n",
      "|       CAMPO GRANDE|\n",
      "|        JOAO PESSOA|\n",
      "|           CASCAVEL|\n",
      "|          GUARULHOS|\n",
      "|          SAO PAULO|\n",
      "|SAO JOSE DOS CAMPOS|\n",
      "|            ITURAMA|\n",
      "|            JACAREI|\n",
      "|             CUIABA|\n",
      "|          FORTALEZA|\n",
      "|     RIO DE JANEIRO|\n",
      "|          SAO PAULO|\n",
      "|           LONDRINA|\n",
      "|              SOUSA|\n",
      "|           CAMPINAS|\n",
      "|           SOROCABA|\n",
      "|           SANTAREM|\n",
      "|          SAO PAULO|\n",
      "|          PESQUEIRA|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframes[13].select('id_municip').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT flag)|\n",
      "+--------------------+\n",
      "|                   1|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data19.select(countDistinct('flag')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will extract the variables for fist analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions used\n",
    "#function to combine all dfs\n",
    "def unionAll(dfs):\n",
    "    return reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs)\n",
    "\n",
    "#function to correct date values\n",
    "def correcting_data(x):\n",
    "    try:\n",
    "        if len(x) == 7:\n",
    "            return x[0:9] + 0 + x[-1]\n",
    "        else:\n",
    "            return x\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to udf all function that will be used\n",
    "udf_correcting_data = udf(correcting_data, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframes[0] = dataframes[0].withColumn(\"DT_SIN_PRI\", df['DT_SIN_PRI'].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a empty list to store results of transformations\n",
    "dfs = []\n",
    "#Standardize each file\n",
    "for j in range(0, len(dataframes)):\n",
    "    df = dataframes[j]\n",
    "    \n",
    "    # Convert columns name to lowercase\n",
    "    for col in df.columns:\n",
    "        df = df.withColumnRenamed(col, col.lower())\n",
    "        \n",
    "    #Change variable names\n",
    "    if \"id_mn_resi\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"id_mn_resi\", \"codmunres\")\n",
    "        \n",
    "    if \"co_mun_res\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"co_mun_res\", \"codmunres\")\n",
    "        \n",
    "    #Group data according to variables used\n",
    "    \n",
    "    #list with columns that will be used to aggregate data\n",
    "    cols_to_group = [\"dt_sin_pri\", \"sem_pri\" ,\n",
    "                 \"dt_notific\", \"sem_not\", \"sg_uf_not\",\n",
    "                 \"classi_fin\", \"criterio\", \n",
    "                 \"id_municip\", \"cs_sexo\", \"cs_gestant\", \"cs_raca\", \"cs_escol_n\",\n",
    "                 \"codmunres\", \"dt_nasc\", \"nu_idade_n\", \"comuninf\"]\n",
    "    \n",
    "    dfs[j] = df.select(cols_to_group).groupBy(cols_to_group).count()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframes[0].select('ID_MN_RESI').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Change variable names\n",
    "        if \"id_mn_resi\" in df.columns:\n",
    "            df = df.withColumnRenamed(\"id_mn_resi\", \"codmunres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(filenames[0], sep =',', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[10].toPandas().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
